---
title: Encoding, Compression & Encryption
authors: Jon Stapleton
date: 4/18/2022
type: page
---

# Encoding & Encryption

Despite the fact that computers represent all computational data and processes using binary numbers, you probably haven't interacted with binary very much, if at all. Binary is a very efficient and compact data format for digital devices, but it isn't "human-readable". Moreover, we don't just use computers to calculate or display numbers; we use them to store and distribute writing, display images and videos, perform complex automated tasks, and all sorts of other things that don't have much to do with binary data.

## A Quick Aside About the IPOS Model of Computing Devices

There are all kinds of computers out there, from desktop computers to smartphones to microprocessors in vehicles. Despite their differences, they all have four things in common:

* They take **input** from users or the environment around them via input devices or sensors
* They **process** that input using a well-defined procedure defined in code
* They can **store** data in a digital format to be recalled later
* They produce human-readable **output** based on stored data

This model of computing is called the **IPOS model** (Input, Processing, Output, Storage). The IPOS model relates to binary because different parts of the model deal with different kinds of data. Computers take **input** and produce **output** in "human-readable" ways, like typing letters on a keyboard or moving a mouse around a desk. Then, they need to *convert* that data from the "human" representation to a binary one for **processing** and **storage** tasks.

<!-- binary-keyboard.jpg -->
![A keyboard with a USB cable coming out of the top, with a label on the left pointing to the keyboard saying Press Button, and a label next to the USB cable saying Send Binary](66767302)

In order for computers to perform these conversions, computer engineers had to design rules that describe how the computer should convert complex human-readable data representations (e.g., text, images, sound) to and from binary numbers.

## Encoding & Protocols

This "conversion" process, where a computer converts human-readable data into binary according to a set of rules, is called **encoding**. The rules that the computer uses to convert that data are called **protocols**. Protocols are like a legend on a map, providing information about how the computer should interpret the numbers that make up binary data in different situations. In this section, you'll read about a couple of protocols in widespread use across many computing systems.

### The ASCII Protocol

One *incredibly* common protocol in widespread use is the **Unicode UTF-8 protocol**, which defines how to convert binary into written text. You can read the entire protocol [here](https://en.wikipedia.org/wiki/Unicode) (but it's not necessary for you to know it by heart or anything like that). Here's a snippet:

| Binary     | Hexadecimal | Decimal | Character |
| ---------- | ----------- | ------- | --------- |
| `01000001` | `41`        | 65      | A         |
| `01000010` | `42`        | 66      | B         |
| `01000011` | `43`        | 67      | C         |
| `01101101` | `6D`        | 111     | m         |
| `01101110` | `6E`        | 112     | n         |
| `01101111` | `6F`        | 113     | o         |

The idea is that each letter you might want to type into a computer gets its own unique number in the form of a **byte**. Computing devices can then use the number to reference letters, allowing them to store text data in a binary format.

> Use the Unicode converter [here](https://toolnanny.com/convert-binary-to-unicode) to convert the following binary information to text:

```diff
010101010110111001101001011000110110111101100100011001010010000001101001011100110010000001110000011100100110010101110100011101000111100100100000011001110111001001100101011000010111010000100001
```

The number-to-letter assignments that make up the Unicode protocol are well-known among computer engineers, allowing them to make technologies that work together even if they were not made by the same company. That's why you can pretty much use any keyboard with your computer--they all use Unicode (or something similar) to transmit text data to your computer!

### Display Protocols

Computers also use binary to store images (well, they use binary to store everything but images are a good example). There are a lot of different protocols for interpreting binary as an image; here's a simplified explanation about how some of them work.

Digital images are made up of pixels, each one a square, single-color area of the image. The more pixels an image has, the more "detail" it can show. An image's "resolution" refers to the number of pixels in the image. "High-resolution" images have lots of pixels, and "low-resolution" images have fewer.

<!-- low-high-res-codie.png -->
![Two cardinals--the one on the right is high-resolution, and the one on the left is low-resolution and pixelated](66767472)

Computers represent each pixel using a binary number. There are a lot of different ways to represent this kind of data with binary; one way is to use a byte (8 bits) to represent the amount of red, green, and blue light in a given pixel. The display device will "read" the binary for each pixel, and change the red, green, and blue lights underneath each pixel to match the binary data. Mixing red, green, and blue (RGB for short) light allows the display to show a whole range of colors.

### Creating Your Own Protocols

Protocols are a very important part of how computers store information. Each protocol defines a couple of things:

1. What should a given chunk of binary data be interpreted to mean?
2. How big should that given chunk of data be (i.e., how many bits)?

Take the Unicode UTF-8 protocol, for instance. The protocol says that when we see binary, we should interpret it as a series of characters, often spelling out words. Importantly, it also says that **each character is 8 bits long.** This part is incredibly important because it defines *how many characters* the Unicode protocol can include in its specifications. Each character is a byte, comprised of eight bits. You can use eight bits to represent any value between zero and two-hundred and fifty-five. That means that the Unicode UTF-8 protocol can only include **two-hundred and fifty-six characters,** one for each value from `00000000`<sub>2</sub> to `11111111`<sub>2</sub>.

If you were to create your own binary protocol, this part of the design would be very important. How many bits would you need to devote to the kind of data your protocol is intended to represent? For example, imagine you needed to write a protocol which assigns a binary number to each American state (of which there are currently fifty). You would need at least 50<sub>10</sub> unique numbers, meaning you would need at least six bits to store the information (e.g., Alaska is `000000`<sub>2</sub>, Wyoming is `110010`<sub>2</sub>). Of course, in this protocol, you'd have 13<sub>10</sub> extra numbers available, so you might as well add the territories to the specification.

Now imagine that you are in charge of designing a database of teachers for a national teachers association; how much space (in bits)would you need to devote to storing what state or territory they're living in? You already know that your "state" protocol uses six bits, so assuming each person can only be from one state at a time, you'd need six bits per teacher in your database just to store information about their state or territory of residence! That can get pretty big pretty quickly; that's 8,000 bits just for 1,000 teachers' state or territory of residence, let alone all the other information you'd want to keep in the database!

## Compression

Some kinds of data, like images, can get to be pretty large. Imagine a 4K 55" television, which has about 80 pixels per square inch of screen. The total area of this size screen is around 1,500 square inches, meaning the display has 121,600 pixels! If each pixel needs 3 bytes of color data, that's over 2 million bits of data (or around 350kB, about the size of a 30-page word document) just to display a still image! Now imagine a streamed video, where the display needs to show 60 images per second! The numbers quickly get to be mind-boggling. Streaming that much data over the internet is a particularly big challenge; often, your internet connection just can't download the data fast enough to play the video!

To solve this problem, computer engineers will often have computers **compress** large amounts of image data by reducing the resolution of the image. Fewer pixels means less binary, which means less data to download and process before sending it to the screen. You might see video compression in action when your internet drops off or slows down, and your YouTube video or video conference images get all "boxy". You can also *hear* compression on a bad connection; you might say the person sounds "like a robot".

<!-- compression-codie.png -->
![A high resolution cardinal on the left, with an arrow pointing to a lower resolution cardinal on the right. The arrow has a label that says compression](66767654)

Computers compress data all the time, for all sorts of reasons and in all sorts of different ways. Sometimes it's to reduce bandwidth (like in the example above), and sometimes its to reduce the size of a file for long-term storage. There are two kinds of compression:

**Lossless Compression:** Lossless compression is when the computer compresses the data by finding patterns in the binary information, and packaging up the data based on those patterns. Data compressed "losslessly" is smaller than its original form, but still contains all the information present in the original data. This compression method is difficult to visualize, but it's pretty common--lots of audio formats (e.g., `.wav` and `.aiff`) are lossless compression formats.

**Lossy Compression:** Lossy compression is when the computer compresses the data by simpifying the data, resulting in new data that's close to the original, but doesn't contain the same information. You've probably seen lossy compression in action if you've ever tried to stream video or audio over a slow internet connection--the image gets all pixelated (lower resolution means less data) and the audio might sound "robotic". Lossy compression is really common on the internet, where the less data you need to transmit the better speed you'll experience.

## Encryption vs Encoding

**Encoding** involves using well-known and transparent processes to interpret binary as human-readable data. **Encryption** is different--it involves using *secret* processes to convert information from an obsfucated format (called **ciphertext**) to a readable format (called **plaintext**). Even though protocols like ASCII are difficult for humans to read, they aren't secret; anyone can look up the ASCII table and "read" binary data if they want to. It's incredibly important for protocols to be public; otherwise, everyone would use a different data format and our computers wouldn't work well together at all.

The process to create **encrypted** data, in contrast, is intentionally designed to make interpreting the encrypted data difficult. Otherwise, anyone would be able to read the data! Encryption is what makes secure and private online communication (e.g., financial transactions, secure messaging) possible.

We don't need to get into all the different kinds of encryption out there for this course, but it is important to cover some of the important tradeoffs associated with encrypting data. In many cases, most of the data on your computer is *not* encrypted, after all--these tradeoffs are often the reason.

* **Reading encrypted data is expensive**: Encrypting and decrypting data is often more computationally taxing than reading un-encrypted (or *plaintext*) data; it's often not worth it to encrypt a lot of information, especially if it needs to get decrypted frequently.
* **Encrypted data takes up more space:** Many times, encrypted data uses up more bits and bytes than plaintext data. You can offset this increase in size by compressing the encrypted data, but that makes the first problem even worse!
* **Encryption isn't foolproof:** Even if your encryption is very powerful, your data is only as safe as the key to unlock that encrypted data. When you need to protect very sensitive data, encryption is just one of many possible security measures you can take (up to and including physical security, e.g., locking your computer up)

---